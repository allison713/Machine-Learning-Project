{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJLklmRrmdbO"
      },
      "outputs": [],
      "source": [
        "# Dependencies to Visualize the model\n",
        "%matplotlib inline\n",
        "from IPython.display import Image, SVG\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr_PnbThmdbP"
      },
      "outputs": [],
      "source": [
        "# Filepaths, numpy, and Tensorflow\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh24reZpmdbQ"
      },
      "outputs": [],
      "source": [
        "# Sklearn scaling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH-6eUxNmdbQ"
      },
      "outputs": [],
      "source": [
        "# Keras Specific\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz2YPgrruVdT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = \"https://dataclass-project4.s3.amazonaws.com/fer2013.csv\"\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2L3xV9uui-k"
      },
      "outputs": [],
      "source": [
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turning the pixels column into a list of arrays, so each image is an array\n",
        "X_values = df['pixels'].tolist()\n",
        "X = []\n",
        "for i in X_values:\n",
        "  X_list = [int(x) for x in i.split(' ')]\n",
        "  X_list = np.asarray(X_list)\n",
        "  X.append(X_list)"
      ],
      "metadata": {
        "id": "OA3VqsnZ1yva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.asarray(X)"
      ],
      "metadata": {
        "id": "xB1-aHTm5HJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_df = pd.get_dummies(df['emotion'])\n",
        "y_df.head()\n",
        "\n",
        "y = []\n",
        "for index, row in y_df.iterrows():\n",
        "    row_array = np.array(row.values)\n",
        "    y.append(row_array)\n",
        "y = np.asarray(y)    "
      ],
      "metadata": {
        "id": "35EFpBs955Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "# Before the model creation, you need to reshape X\n",
        "# Assuming that you have grayscale images of size 48x48\n",
        "X = X.reshape(-1, 48, 48, 1)  # reshaping into (num_images, 48, 48, 1)\n",
        "\n",
        "# Now split your data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Rescale the images\n",
        "X_train_scaled = X_train / 255\n",
        "X_test_scaled = X_test / 255\n"
      ],
      "metadata": {
        "id": "_2Gbj3DS8pkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,       # Rotate images randomly within the range [-20, 20] degrees\n",
        "    width_shift_range=0.2,   # Shift the width of the images randomly by up to 20% of the total width\n",
        "    height_shift_range=0.2,  # Shift the height of the images randomly by up to 20% of the total height\n",
        "    shear_range=0.2,         # Apply shear transformations randomly within the range [-0.2, 0.2]\n",
        "    zoom_range=0.2,          # Randomly zoom into images by up to 20%\n",
        "    horizontal_flip=True,    # Flip images horizontally\n",
        "    fill_mode='nearest'      # Fill any newly created pixels after rotation or width/height shift\n",
        ")"
      ],
      "metadata": {
        "id": "SBYO2GRzjo6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a generator for training data\n",
        "train_generator = datagen.flow(X_train_scaled, y_train, batch_size=32)\n",
        "# Create a generator for validation data\n",
        "validation_generator = datagen.flow(X_test_scaled, y_test, batch_size=32)"
      ],
      "metadata": {
        "id": "UQzEeScRjvd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #First Build\n",
        "# from keras.models import Sequential\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Activation, Flatten\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(64,kernel_size=(4, 4), activation='relu', input_shape=(48,48,1), data_format=\"channels_last\"))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv2D(64*2,kernel_size=(4, 4), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv2D(64*4,kernel_size=(4, 4), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Conv2D(64*8,kernel_size=(4, 4), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# model.add(Flatten())\n",
        "\n",
        "# model.add(Dense(units=7, activation='softmax'))"
      ],
      "metadata": {
        "id": "JIIHdCRo1jSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Second Build\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Activation, Flatten\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(64,kernel_size=(3, 3), activation='relu', input_shape=(48,48,1), data_format=\"channels_last\"))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(64*2,kernel_size=(3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(64*4,kernel_size=(3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(64*8,kernel_size=(3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Flatten())\n",
        "\n",
        "# model.add(Dense(8*64, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# #output layer\n",
        "# model.add(Dense(units=7, activation='softmax'))"
      ],
      "metadata": {
        "id": "LjDmzooY2Vgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Third Build\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Activation, Flatten\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(64,kernel_size=(3, 3), activation='relu', input_shape=(48,48,1), data_format=\"channels_last\"))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(64*2,kernel_size=(3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(64*4,kernel_size=(3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Conv2D(64*8,kernel_size=(3, 3), activation='relu'))\n",
        "# model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# model.add(Flatten())\n",
        "\n",
        "# model.add(Dense(8*64, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(4*64, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "# model.add(Dense(2*64, activation='relu'))\n",
        "# model.add(Dropout(0.4))\n",
        "\n",
        "# #output layer\n",
        "# model.add(Dense(units=7, activation='softmax'))"
      ],
      "metadata": {
        "id": "X1nfwbo528cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Activation, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(64,kernel_size=(3, 3), activation='relu', input_shape=(48,48,1), data_format=\"channels_last\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64*2,kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64*2,kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64*4,kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64*4,kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(64*8,kernel_size=(3, 3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64*8,kernel_size=(3, 3), activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(8*64, activation='elu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(4*64, activation='elu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(2*64, activation='elu'))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "#output layer\n",
        "model.add(Dense(units=7, activation='softmax'))"
      ],
      "metadata": {
        "id": "0wU6XxaK9Rp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.summary()"
      ],
      "metadata": {
        "id": "RAHfjCuGWh8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='Adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "IS08K4Gt9S0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    shuffle=True,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "juqBKWVZ9UNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model\n",
        "model_loss, model_accuracy = model.evaluate(\n",
        "    X_test_scaled, y_test, verbose=2)\n",
        "print(\n",
        "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
        "\n",
        "# 1st trial - Loss: 1.9519965648651123, Accuracy: 0.6375035047531128\n",
        "# Dropout 20% on each layer\n",
        "# 2nd trial - Loss: 1.2198774814605713, Accuracy: 0.6517135500907898\n",
        "# Dropout 40% on each layer\n",
        "# 3rd trial - Loss: 1.1788020133972168, Accuracy: 0.6213430166244507\n",
        "# Added three dense layers\n",
        "# 4th trial - Loss: 1.1446014642715454, Accuracy: 0.5862357020378113\n",
        "# Changed activation functions from relu to tanh\n",
        "# 5th trial - Loss: 1.1010137796401978, Accuracy: 0.6258010864257812\n",
        "# Changed activation functions from tanh to elu\n",
        "# 6th trial - Loss: 1.3327525854110718, Accuracy: 0.6281694173812866\n",
        "# Removed dense layers and changed convolution layers from relu to elu\n",
        "# 7th trial - Loss: 1.3180416822433472, Accuracy: 0.6278907656669617\n",
        "# Changed to dropout to 50% on each layer, slow learning rate\n",
        "# 8th trial - \n",
        "# Using Data Augmentation, returned learning rate to default."
      ],
      "metadata": {
        "id": "xHh5c-M29VYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model\n",
        "model.save(\"emotion_model.h5\")"
      ],
      "metadata": {
        "id": "IhihlD959WSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the model\n",
        "from tensorflow.keras.models import load_model\n",
        "emotion_model = load_model('emotion_model.h5')"
      ],
      "metadata": {
        "id": "x4HXrif2-XYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making predictions\n",
        "predicted = emotion_model.predict(X_test_scaled)\n",
        "predicted = np.argmax(predicted, axis=1)\n",
        "predicted"
      ],
      "metadata": {
        "id": "dSMfDe_C-bK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe of the predictions\n",
        "predicted_df = pd.DataFrame(predicted)\n",
        "predicted_df = predicted_df.rename(columns={0: \"Predicted\"})\n",
        "predicted_df.head()"
      ],
      "metadata": {
        "id": "OuzFgd0I-cwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe of the actual values\n",
        "y_test_actual = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Creating a dataframe of the actual values\n",
        "y_test_df = pd.DataFrame(y_test_actual, columns=[\"Actual\"])\n",
        "\n",
        "y_test_df.head()"
      ],
      "metadata": {
        "id": "codPIX9A-don"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the two dataframes\n",
        "merged_df = pd.merge(y_test_df, predicted_df, left_index=True, right_index=True)\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "yylx8FGk-ef6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataframe of the emotions\n",
        "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "emotions_df = pd.DataFrame(emotions)\n",
        "emotions_df = emotions_df.rename(columns={0: \"Emotions\"})\n",
        "emotions_df.head()"
      ],
      "metadata": {
        "id": "fiy3e6tv-fll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the emotions dataframe with the merged dataframe\n",
        "merged_df = pd.merge(merged_df, emotions_df, left_on='Actual', right_index=True)\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "l1_KZgoK-fiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging the emotions dataframe with the merged dataframe\n",
        "merged_df = pd.merge(merged_df, emotions_df, left_on='Predicted', right_index=True)\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "8_yYwOPM-iNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}